\chapter{Generative Adversarial Networks - Implémentation}

 La structure théorique des \textbf{Generative Adversarial Networks} étant maintenant définie, nous devons implémenter ces réseaux. Pour cela, nous nous sommes lancés dans la continuité de MNIST, avec la génération de chiffres manuscrits par le \textbf{Générateur}, qui doit alors convaincre le \textbf{Discriminateur}. Ce cas de figure est idéal pour une première implémentation des GAN, puisque nous restons avec des données relativement simple (Dimension 28*28, en niveau de gris), avec un format normalisé et une importante base de données à disposition (La base de données MNIST : 60000 images d'apprentissages, 10000 données de test).

Le \textbf{Discriminateur} s’entraîne donc à reconnaître les chiffres provenant de la base de données de MNIST en les différenciant des chiffres factices créés par le \textbf{Générateur}. Dans un premier temps, nous travaillons sur des chiffres uniques, en apprenant au Discriminateur à reconnaître uniquement de 3 par exemple.

Nous verrons dans ce chapitre tout d'abord l'implémentation de notre GAN, puis les premiers résultats obtenus. Ces résultats nous ont confronté à divers problèmes, que nous aborderons alors avec les différentes solutions envisagées.


\section{Implémentation des \textit{Generative Adversarial Networks}}
\subsection{Présentation de la structure}

La structure de notre \textit{Generative Adversarial Networks} diffère assez peu de ce que nous avons implémenté pour MNIST : on suit le même principe, la classe \textit{Application} ne possédant plus un réseau mais les deux réseaux de neurones nécessaires au GAN. 
Ainsi, une classe \textit{Teacher} se charge d'apprendre aux deux réseaux en fonction des paramètres que l'on définit dans un fichier de configuration json. Enfin, pour une plus grande souplesse dans la création de nos réseaux, nous avons implémenté divers types de couches de neurones, avec NeuronLayer classe abstraite mère, dont découlent les couches bruitées (NoisyLayer), les couches de perceptrons classiques (FullConnectedLayer), et les différentes couches à convolution (traitées dans le chapitre suivant).

\subsection{Taille des réseaux - Influence}
\paragraph{Nombre de couches}
Les réseaux de nombre de couches complètement connectées inférieur ou égal à 2 sont majoritairement inefficaces. Des réseaux à 3 couches ont souvent été suffisants. Rajouter davantage de couches a peu d'influence, si ce n'est que le temps de calcul de la rétropropagation est grandement augmenté. Pour modifier davantage le comportement du réseau, il est nécessaire de recourir aux DCGANs (voir \ref{DCGAN}).
\paragraph{nombre de neurone par couche}
Des couches avec plus de 1000 neurones dans une couche ont souvent été peu efficaces. On a utilisé des couches avec souvent entre 50 et 800 neurones par couche. 

\subsection{Ratio d'apprentissage- Influence}

\paragraph{En théorie :}
On a vu dans la partie précédente l'importance théorique de l'équilibre entre les deux réseaux : en effet, l'état idéal correspond à l'équilibre de Nash, où le générateur et le discriminateur sont tous deux au même niveau, et si l'un progresse, cela sera au détriment de l'autre (voir partie théorique sur les GAN). Cela correspondrait au niveau des scores que le générateur répondrait 0.5 à toutes les demandes : il ne saurait pas si l'image est réelle ou fictive. Le générateur ne serait pas encore assez fort pour entièrement tromper le discriminateur, mais il ne serait pas non plus complètement démasqué par le discriminateur. \newline
Cet état d'équilibre devrait permettre aux deux réseaux d'apprendre simultanément. Pour équilibrer le système et atteindre cet état, un paramètre sur lequel il serait intéressant de jouer est le ratio d'apprentissage : lors d'un cycle d'apprentissage, combien de fois devons-nous faire apprendre le générateur puis combien de fois devons-nous entrainer le discriminateur ? 
Il y a deux aspects à prendre en compte pour ce paramètre : tout d'abord, adapter cette cadence d'apprentissage permet de rééquilibrer les deux réseaux. De plus, il peut être intéressant d'entrainer le générateur un grand nombre de fois par rapport à un discriminateur fixé, permettant alors un apprentissage plus précis. Le générateur essaiera alors de viser une cible fixe, et apprendra de manière plus efficace.

\paragraph{En pratique :}
En pratique, ce paramètre a été très difficile à adapter : en effet, l'équilibre de Nash a été en pratique impossible à obtenir, le générateur étant la majorité du temps largement au dessus du générateur. De plus, le système est trop instable pour permettre un tel équilibre. Ainsi, la cadence d'apprentissage ne nous permettait que d'aider le générateur, lui permettant de garder le rythme un peu plus longtemps dans la course contre le discriminateur.\newline
\\
Nous verrons d'autres méthodes qui jouent directement sur ces apprentissages, et qui permettent alors d'aider le générateur à chaque itération, dans la partie suivante, ce problème étant récurrent dans notre mise en place des GANs.

\subsection{Taille des entrées - Influence}

Les entrées, qui consistent en du bruit $\in [-1,1]$ doivent être suffisamment nombreuses. Bien qu'une entrée de bruit soit réelle (double flottant dans l'implémentation) et qu'elle devrait en théorie correspondre à une infinité d'entrées, les résultats \textsuperscript{ref. nécessaire} montrent que le nombre d'entrées doit être idéalement au moins aussi grande que le nombre de sorties.
Une solution couramment adoptée est l'utilisation d'entrées, donc de bruit, sur les autres couches du générateur. On aura alors une entrée sur la première couche du réseau de neurones, et les autres couches peuvent alors être bruitées \cite{goodfellow_nips_2016}. Nous traiterons cette méthode dans ce chapitre sur le point du \textit{Mode Collapse} du Générateur.

\section{Premiers résultats}

Les résultats initiaux pour les GANs furent très encourageants. Les courbes de score du générateur (voir \ref{notations} pour la définition du score) avaient toujours la même forme, tandis que la courbe du discriminateur était parfois symétrique par rapport à la droite 0,5, mais pas dans tous les cas. On fait figurer ici quelques-uns de nos résultats initiaux
\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.5\textwidth]{images/18_01_12-GAN/image.png}\caption{Chiffre 8 généré un mois après l'implémentation effective du GAN - le temps de trouver des paramètres de réseau qui fonctionnent}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{images/18_01_12-GAN/courbe.png}\caption{Apprentissage du Générateur associée au chiffre 8 - on affiche le score du générateur $D(G(z))$}
\end{center}
\end{figure}

\paragraph{affichage successif d'un chiffre à différents moments de l'apprentissage}

\begin{figure}[h!]
\begin{center}
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_01_17-GAN/image_5000.png}
\centering\\5000 apprentissages
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_01_17-GAN/image_10000.png}
\centering\\10000 apprentissages
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_01_17-GAN/image_15000.png}
\centering\\15000 apprentissages
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_01_17-GAN/image_50000.png}
\centering\\50000 apprentissages
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_01_17-GAN/image_90000.png}
\centering\\90000 apprentissages
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_01_17-GAN/image_95000.png}
\centering\\95000 apprentissages
\endminipage\hfill
\caption{affichage successif du chiffre 6 à différents moments de l'apprentissage}
\label{fig:res2}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{images/18_01_17-GAN/courbe.png}\caption{Apprentissage du Générateur associée au chiffre 6 - on affiche le score du générateur $D(G(z))$}
\end{center}
\end{figure}

\subsection{Comparaison des fonctions de coût du générateur}
Différentes fonctions de coût peuvent être utilisées pour le Générateur. Nous avons implémenté le MinMax, l'heuristique non saturante et le maximum de vraisemblance (aussi appelée KL-Divergence), et les avons comparées. On affiche en figure \ref{fig:coutgen-courbe} le score du discriminateur et générateur pour chacune des fonctions de coût du générateur. On montre à titre de comparaison le meilleur 3 obtenu pour chaque fonction de coût à la figure \ref{fig:coutgen-image}. Le meilleur 3 a à chaque fois été obtenu dans la zone de plus forte variation. Avant le réseau n'avait pas appris, après le réseau était passé en mode collapse (voir \label{mode-collapse} pour plus d'explications). Dans le cas de l'heuristique non-saturante, l'image est prise proche au premier pic, qui est un maximum global du score du générateur.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1\textwidth]{images/18_02_22-GAN/Comparaison_heu-minmax-kl/courbe-comparaison-dis.png}\\Score du discriminateur $D(x)$
\end{center}
\end{figure}
\begin{figure}[h!]
\begin{center}
\includegraphics[width=1\textwidth]{images/18_02_22-GAN/Comparaison_heu-minmax-kl/courbe-comparaison-gen.png}\\Score du générateur $D(G(z))$
\caption{Heuristique non saturante en bleue, KL en rouge, MinMax en jaune}
\label{fig:coutgen-courbe}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_02_22-GAN/Comparaison_heu-minmax-kl/image-heuristique.png}
\centering\\Heuristique non saturante
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_02_22-GAN/Comparaison_heu-minmax-kl/image-kl_divergence.png}
\centering\\KL-Divergence
\endminipage\hfill
\minipage{0.32\textwidth}
\includegraphics[width=1\textwidth]{images/18_02_22-GAN/Comparaison_heu-minmax-kl/image-minmax.png}
\centering\\MinMax
\endminipage\hfill
\caption{Comparaison de la qualité d'image pour 3 fonctions de coût du générateur différentes}
\label{fig:coutgen-image}
\end{center}
\end{figure}

On remarque que la KL-divergence et le MinMax prennent beaucoup de temps à se mettre en place et collapse rapidement, tandis que l'heuristique non-saturante permet d'obtenir un résultat satisfaisant très tôt dans l'apprentissage.
Nous avons effectué pour ces raisons par la suite la majorités des tests avec l'heuristique non-saturante.

\section{Difficultés rencontrées et solutions émises}

Au cours du développement des GANs, nous avons été confrontés à de très nombreuses difficultés, que nous avons tenté de répertorier avec les solutions envisagées. À l'heure actuelle, certains problèmes sont toujours sans solution probante et restent des domaines de recherches ouverts : c'est le cas en particulier du \textbf{mode collapse} que nous détaillons ci-après.

\subsection{Le problème d'évaluation des performances}

Le premier et plus grand problème que nous avons rencontré est \textbf{l'absence de méthode systématique d'évaluation des performances d'un réseau donné.}

En effet, pour qu'un protocole expérimental puisse être mené à bien, il faut comparer un résultat témoin avec les résultats pour lesquels des paramètres ont varié, et déduire de la différence des performances si le paramètre testé est signifiant, et dans quelle mesure.

Ce protocole est \textbf{fondamental} en recherche expérimentale, et il est \textbf{inapplicable} ici. En effet, nous n'avons trouvé aucun moyen de mesurer la qualité d'une image, à l'exception de la mesure à l’œil par les membres de l'équipe (ce qui ne peut être qualifié de méthode rigoureuse et systématique d'évaluation). 

\subsubsection{Les données à disposition pour mesurer les performances}

Afin de bien cerner le problème, résumons ce que nous avons à disposition à la fin d'une expérience. Si dans certains cas nous avons fait des essais nous permettant de mesurer d'autres grandeurs que les suivantes (comme la valeur des poids etc), seules les deux ci-après sont accessibles systématiquement, pour chaque expérience.
Nous disposons donc de :

\begin{itemize}
  \item Un ensemble d'images de synthèse générées à différents instants d'apprentissage; par exemple nous pouvons demander à produire 100 images tous les 10 apprentissages, ce qui permet de suivre assez finement l'évolution du GAN au fur et à mesure de son apprentissage
  \item Les courbes de score des deux réseaux, générées à la fin de l'expérience. La courbe de score d'un réseau représente la proportion d'erreurs dans toutes les réponses qu'il donne. Ainsi la courbe de score du discriminateur compte le nombre de bonnes réponses du réseau à des entrées aussi bien de synthèse (réponse attendue : 0) que réelles (réponse attendue : 1); la courbe du générateur est calculée en fonction des réponses du discriminateur aux images de synthèse. Ces courbes sont graduées en abscisse par rapport à nombre d'apprentissages effectués.
\end{itemize}

\paragraph{Sur l'utilisation des courbes de score}

À première vue, ces courbes paraissent être un indicateur pertinent de la qualité des réseaux. Elles permettent \textbf{en théorie} :
\begin{itemize}
  \item d'observer l'équilibre de Nash (qui, en pratique, n'a jamais été observé)
  \item d'observer les déséquilibres entre les deux réseaux afin d'affiner les paramètre d'équilibre sur les expériences suivantes
  \item de comparer les images produites avec les scores des réseaux au moment de la génération de l'image
\end{itemize}

En pratique, nous n'avons jamais réussi à interpréter efficacement ces courbes. Leur étude représente une perte pure de temps, temps qui aurait dû être alloué à la recherche d'une méthode d'évaluation efficace. Les courbes paraissent extrêmement instables, on peut observer des formes récurrentes mais très difficiles à expliquer (chute brutale de l'une des deux malgré une bonne qualité d'image, etc), et par dessus tout nous n'avons pas réussi à mettre en lumière des corrélations entre les paramètres choisis et les courbes résultantes.

Enfin, nous avons été incapables de trouver des corrélations pertinentes entre la qualité des images et les courbes de score. Nous avons au mieux été capables de trouver des explications à posteriori sur les liens qui peuvent exister; c'est à dire que nous n'avons jamais pu prévoir (de manière reproductible et indépendante du hasard) la qualité des images à partir de la seule analyse des courbes.

\paragraph{Sur l'analyse des échantillons d'images}

Deux problèmes principaux s'opposent à l'analyse 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Compléter 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Évaluation par réseau(x) tiers}

Une première idée pour l'évaluation, serait de passer les images de synthèses dans un 3\textsuperscript{ème} réseau (appelons le évaluateur, mais il se comporte exactement comme le discriminateur) qui aurait appris à part, en dehors de la structure de GAN et qui serait un juge objectif sur les résultats du GAN. Ce réseau aurait été entraîné sur les données réelles, et resterait immuable une fois son apprentissage terminé. 

Cette méthode est en pratique inapplicable (et inappliquée, pour ce que nous en savons), notamment à cause de la difficulté à équilibrer ce 3\textsuperscript{ème} réseau avec les deux autres; équilibrer un GAN est déjà une tâche particulièrement ardue (nous y reviendrons), ajouter cet évaluateur la rend insurmontable. Car si l'évaluateur est très performant, il risque de donner une performance nulle à tous les générateurs qu'il testera; à l'inverse si on l'a délibérément fait peu apprendre, il ne permettra pas de discriminer efficacement les générateurs qu'il testera.

On pourrait envisager de créer une échelle d'évaluateurs, constituée de plusieurs évaluateurs ayant chacun arrêté leur apprentissage à un moment différent, afin de pouvoir graduer les résultats obtenus par les générateurs sur une échelle de performance. Nous n'avons pas nous même implémenté cette technique et n'avons pas trouvé d'articles faisant mention de l'application de cette méthode, mais elle a au moins l'avantage majeur d'être rigoureuse, systématique et indépendante du jugement humain.

\subsection{Mode Collapse du \textit{Générateur} vers un unique point}
\subsubsection{Présentation du problème}
\label{mode-collapse}

Un problème récurrent que l'on peut observer dans les GAN est le \textit{mode collapse}, ou \textit{missing modes}. Prenons un GAN, le générateur doit tenter de créer des images les plus réalistes possible par rapport à une distribution de données réelles, par l'intermédiaire du discriminateur. Cette distribution peut présenter plusieurs modes, plusieurs zones en quelque sorte (En reprenant l'exemple des chiffres de MNIST, on peut vouloir générer des 3, ou des 4). 
\\ \\
Cependant, on observe que le générateur a souvent tendance à apprendre à ne générer qu'un seul mode. On appelle cela le mode collapse : le générateur apprendra à aller vers une des zones de la distribution voulue, se rendra compte que cette zone marche, et y restera. 
Sur l'exemple simplifié ci-dessous, la distribution d'origine présente 4 zones, et le GAN n'en couvre qu'une seule.
Le discriminateur apprendra alors à se méfier de cette zone, puisque, même si elle est très proche de la distribution d'origine, il apprend au fur et à mesure à reconnaître les images du générateur. Ainsi, cette zone est peu à peu considérée comme artificielle pour le discriminateur, et le générateur va collapse sur une autre zone considérée comme valide. On aura alors un jeu de rebonds, le générateur collapse sur des zones qui seront alors désapprouvées par le discriminateur, et se déplacera vers une autre zone. 
On a donc un cercle qui fera que le générateur sera et restera dans un état de collapse. 
Il faut alors trouver des méthodes pour empêcher le générateur de ne générer qu'une distribution très restreinte (un mode).

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{images/missing_modes.png}\caption{Missing modes : les points représentent les éléments de la distribution de données réelles. En bleu l'ensemble de sortie du GAN appartenant à la distribution réelle, et en rouge les zones de la distribution initiale non couverte par le GAN.}
\end{center}
\end{figure} 

Ce phénomène récurrent est observable à différents degrés : on peut avoir des collapses partiels, les données générées seront donc variables mais toute la distribution ne sera pas représentée par le générateur, ou des collapses totaux, où le générateur ne sera qu'une projection vers un point. Les données générées seront alors très proches les unes des autres.
Nous avons été confrontés à ce problème de collapse total en générant des chiffres à partir de la base de données MNIST. Comme montré ci-dessous, en essayant de générer des 3, nous obtenons pour un réseau donné, des images générées qui sont toutes très proches, avec la même forme de 3, les mêmes pixels singuliers.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{images/missing_modes_2.png}\caption{Missing modes : On obtient pour un même réseau, 3 échantillons générés qui sont sensiblement les mêmes.}
\end{center}
\end{figure}


\subsubsection{Solution envisagée : Retrait du biais dans les réseaux (non fructueuse)}

Une première hypothèse concernant ce problème de Mode Collapse était la suivante : l'unique image produite par le générateur n'était en réalité qu'une image du masque de biais qui figeait l'image, par exemple en plaçant certains biais trop haut ou trop bas, annulant les entrées bruitées en début de réseau. On aurait alors une image déterminée uniquement par les biais, ce qui expliquerait un collapse vers une unique image.
De plus, en observant précisément le masque de biais sous forme matricielle, on remarque que ses poids forment effectivement l'image d'un nombre, comme on peut le voir ci-dessous :

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{images/masqueBiais.png}\caption{Masque du biais : on peut observer que le biais forme lui même les bases d'une image, un 3 ici.}
\end{center}
\end{figure}

On peut vérifier cette supposition en supprimant les biais de nos réseaux de neurones. On aurait alors plus de valeurs constantes qui figeraient l'image. Cependant, en implémentant ces réseaux, on n'observe qu'un ralentissement de l'apprentissage vers un état de Collapse similaire : les résultats sont moins intéressants (chiffres moins beaux), l'apprentissage est plus lent, et on converge toujours vers un Mode Collapse.

La suppression du biais ne permet donc pas de résoudre ce problème, c'est bien la fonction de transfert globale du réseau de neurones qui converge vers cet unique point.  


\subsubsection{Solution envisagée : Injection de bruit dans chaque couche du Générateur (fructueuse)}

Une solution est proposée par Ian Goodfellow dans son article dans NIPS 2016 intitulé \textbf{Tutorial : Generative Adversarial Networks} \cite{goodfellow_nips_2016}. La structure du générateur est assez libre dans l'architecture des GAN, ainsi, on peut placer des entrées (du bruit) au début du réseau, mais aussi sur n'importe quelle couche de celui-ci notamment, sans réellement compromettre les performances. En effet, injecter des entrées de bruit dans des couches intermédiaires revient à faire des entrées classiques qui traverseront moins de couches de neurones. Cependant, le bruit au niveau de la dernière couche semble perturber le réseau et l'empêche de tomber dans un mode collapse. 

Cela peut s'expliquer par le fait que les entrées bruitées perturbent la dernière couche du réseau, l'empêchant de se stabiliser sur une unique sortie trompant le discriminateur. Cette perturbation va forcer le générateur à diversifier ses sorties.
Une autre hypothèse serait que la dernière couche ne peut pas différencier les entrées provenant des couches antérieures des entrées de bruit si celles-ci sont suffisamment nombreuses. Ainsi, il sera obligé de toutes les prendre en compte (il ne peut pas juste annuler les poids correspondant) ce qui générera de la variabilité dans la sortie, puisqu'une partie non négligeable des entrées n'est que du bruit.

Cependant on peut observer dans des réseaux bruités, pour un même nombre d'apprentissages, des patterns généraux dans chaque image, ce qui nous montre un mode collapse partiel mais très léger comparé au précédent. De tels réseaux ont de plus besoin de beaucoup plus d'apprentissages pour donner des résultats convaincants. Cette solution nous permet donc de limiter partiellement ce problème de mode collapse.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{images/gan_bruite.png}\caption{Mode collapse éliminé : pour un même nombre d'apprentissages (8500 ici) on obtient un 3 et un 7 à l'aide d'un réseau bruité}
\end{center}
\end{figure}


